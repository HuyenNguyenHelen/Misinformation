{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a8a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4dba1",
   "metadata": {},
   "source": [
    "# Merging metadata files\n",
    "- Merging metadata files collected from four databases\n",
    "- Output: a single df with the wanted fields: 'title', 'authors', 'venue', 'year', 'citationCount', 'fieldsOfStudy', 'abstract', 'doi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b3b02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disinformation\n",
      "fake+news\n",
      "spam\n",
      "troll\n",
      "all_dfs.shape (4100, 16)\n",
      "misinformation\n",
      "rumor\n",
      "dfs.shape (2119, 16)\n",
      "the SD concatenated data has 6219 examples, and 16 columns.\n",
      "Index(['Item type', 'Authors', 'Title', 'Journal', 'Publication year',\n",
      "       'Volume', 'Issue', 'Pages', 'Date published', 'ISSN', 'URLs', 'DOI',\n",
      "       'Abstract', 'Keywords', 'Notes', 'query'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Science Direct\n",
    "\n",
    "# Collected Files exist in the folders:\n",
    "# raw_ScienceDirect____ ScienceDirect___________ SD Disinfo: 10 files\n",
    "#                   |                  |_____ SD fake news: 10 files\n",
    "#                   |                  |_____ SD Spam: 10 files\n",
    "#                   |                  |_____ SD troll: 10 files\n",
    "#                   |__ ScienceDirect______ metadata_ScDr_misinformation.csv\n",
    "#                                      |__ metadata_ScDr_rumor.csv\n",
    "\n",
    "\n",
    "def merge_SD_multi(root_p, wanted_fields, wanted_size):\n",
    "    query_name = {\n",
    "        'SD Disinfo': 'disinformation',\n",
    "        'SD fake news': 'fake+news',\n",
    "        'SD Spam' : 'spam',\n",
    "        'SD troll': 'troll'\n",
    "    }\n",
    "    all_data = []\n",
    "    for fold in os.listdir(root_p): \n",
    "        print(query_name[fold])\n",
    "        data = []\n",
    "        for file_p in glob.glob('%s\\%s\\*.csv'%(root_p, fold)):\n",
    "#             print(file_p)\n",
    "            try:\n",
    "                with open(file_p, 'r') as f:\n",
    "                    df = pd.read_csv(f)\n",
    "            except:\n",
    "                with open(file_p, 'r', encoding = 'utf-8') as f:\n",
    "                    df = pd.read_csv(f)        \n",
    "            data.append(df)\n",
    "            dfs = pd.concat(data)\n",
    "            dfs['query'] = query_name[fold]\n",
    "            \n",
    "            if wanted_size is not None:\n",
    "                wanted_size = wanted_size\n",
    "            else:\n",
    "                wanted_size = len(dfs)\n",
    "#             print('df.shape', dfs.shape)        \n",
    "\n",
    "            all_data.append(dfs[wanted_fields][:wanted_size])\n",
    "\n",
    "    all_dfs = pd.concat(all_data)\n",
    "    print('all_dfs.shape', all_dfs.shape)\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "def merge_SD_single(root_p, wanted_fields, wanted_size):\n",
    "    data = []\n",
    "    for file_p in glob.glob('%s\\*.csv'%(root_p)):\n",
    "        query = file_p.split('\\\\')[-1].split('_')[-1].split('.')[0]\n",
    "        print(query)\n",
    "        try:\n",
    "            with open(file_p, 'r') as f:\n",
    "                df = pd.read_csv(f)\n",
    "        except:\n",
    "            with open(file_p, 'r', encoding = 'utf-8') as f:\n",
    "                df = pd.read_csv(f)\n",
    "        df['query'] = query\n",
    "        \n",
    "        if wanted_size is not None:\n",
    "                wanted_size = wanted_size\n",
    "        else:\n",
    "            wanted_size = len(df)\n",
    "        data.append(df[wanted_fields][:wanted_size])\n",
    "    dfs = pd.concat(data)\n",
    "    print('dfs.shape', dfs.shape)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wanted_fields = ['Item type', 'Authors', 'Title', 'Journal', 'Publication year',\n",
    "       'Volume', 'Issue', 'Pages', 'Date published', 'ISSN', 'URLs', 'DOI',\n",
    "       'Abstract', 'Keywords', 'Notes', 'query']\n",
    "    root = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\"\n",
    "    out_p = root + '\\merged_ScienceDirect'\n",
    "        \n",
    "    sd_multi_f = merge_SD_multi(\n",
    "        root + \"\\\\raw_ScienceDirect\\\\ScienceDirect\", wanted_fields, wanted_size = None) \n",
    "    sd_single_f = merge_SD_single(\n",
    "        root + \"\\\\raw_ScienceDirect\\\\Science_direct\", wanted_fields, wanted_size = None)\n",
    "    concat_SD = pd.concat([sd_multi_f, sd_single_f])\n",
    "    print('the SD concatenated data has %d examples, and %d columns.'%(concat_SD.shape[0], concat_SD.shape[1]))\n",
    "    print(concat_SD.columns)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(out_p, exist_ok = True)\n",
    "    except OSError as error:\n",
    "        print('Directory cannot be created!')\n",
    "    with open(out_p + '\\ScienceDirect.csv', 'w', encoding = 'utf-8', newline = '') as f:\n",
    "        concat_SD.to_csv(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3692ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____scopus_____\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 897\n",
      "scopus 11111 has 897 instances.\n",
      "scopus 2222 has 897 instances.\n",
      "\n",
      "_____semantic_scholar_____\n",
      "wanted_size: 1000\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "\n",
      "_____wos_____\n",
      "wanted_size: 1000\n",
      "wos 11111 has 1813 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 3584 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 6313 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 3755 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 2493 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 1698 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "\n",
      "_____science_direct_____\n",
      "Index(['Unnamed: 0', 'Item type', 'Authors', 'Title', 'Journal',\n",
      "       'Publication year', 'Volume', 'Issue', 'Pages', 'Date published',\n",
      "       'ISSN', 'URLs', 'DOI', 'Abstract', 'Keywords', 'Notes', 'query'],\n",
      "      dtype='object')\n",
      "\n",
      "Done! Merged df has 24056 examples, and 10 fields\n"
     ]
    }
   ],
   "source": [
    "def merge_cross_DBs(root_p, wanted_fields, wanted_size=1000):\n",
    "    fold_names = {\n",
    "        'scopus': 'Scopus csv',\n",
    "        'semantic_scholar': \"Semantic_scholar\",\n",
    "        'wos': \"WoS_Metadata\",\n",
    "        'science_direct': 'merged_ScienceDirect'\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "    for k, v in fold_names.items():\n",
    "        print('\\n_____%s_____' % k)\n",
    "        fold_p = root_p + \"\\\\\" + v\n",
    "        file_ps = [f for f in glob.glob(fold_p + '\\*.csv')]\n",
    "        if k == 'science_direct':\n",
    "            if len(file_ps) == 1:\n",
    "                with open(file_ps[0], 'r', encoding='utf-8') as f:\n",
    "                    df = pd.read_csv(f)\n",
    "            else:\n",
    "                print('more than one file in the directory')\n",
    "            print(df.columns)\n",
    "            df.columns = ['Unnamed: 0', 'Item type', 'authors', 'title', 'venue', 'year', 'Volume', 'Issue', 'Pages',\n",
    "                          'Date published', \n",
    "                          'ISSN', 'URLs', 'doi', 'abstract', 'Keywords', 'Notes', 'query']\n",
    "            df['database'] = k\n",
    "            new_cols = list(set(wanted_fields).difference(list(df.columns)))\n",
    "            for col in new_cols:\n",
    "                df[col] = ''\n",
    "            df = df[wanted_fields]\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            for path in file_ps:\n",
    "                try:\n",
    "                    with open(path, 'r') as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                except:\n",
    "                    with open(path, 'r', encoding='utf-8') as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                if wanted_size < len(df):\n",
    "                    wanted_size = 1000\n",
    "                else:\n",
    "                    wanted_size = len(df)\n",
    "                print('wanted_size: %d'%wanted_size)\n",
    "                \n",
    "                if k == 'scopus':\n",
    "                    df.columns = ['Item type', 'authors', 'title', 'venue', 'year', 'Volume',\n",
    "                                  'Issue', 'Pages', 'Date published', 'URLs', 'doi', 'Notes', 'citationCount']\n",
    "\n",
    "                    query = path.split('\\\\')[-1].split(' ')[-1].split('.')[0]\n",
    "                elif k == 'wos':\n",
    "                    df.columns = ['Publication Type', 'authors', 'Book Authors', 'Book Editors',\n",
    "                                  'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
    "                                  'Group Authors', 'title', 'Source Title', 'Book Series Title',\n",
    "                                  'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
    "                                  'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
    "                                  'Conference Host', 'Author Keywords', 'Keywords Plus', 'abstract',\n",
    "                                  'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
    "                                  'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
    "                                  'Funding Text', 'Cited References', 'Cited Reference Count',\n",
    "                                  'citationCount, WoS Core', 'Times Cited, All Databases',\n",
    "                                  '180 Day Usage Count', 'Since 2013 Usage Count', 'venue',\n",
    "                                  'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
    "                                  'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
    "                                  'year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
    "                                  'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
    "                                  'Article Number', 'doi', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
    "                                  'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
    "                                  'fieldsOfStudy', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
    "                                  'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
    "                                  'UT (Unique WOS ID)', 'Web of Science Record']\n",
    "                    query = path.split('\\\\')[-1].split('_')[-1].split('.')[0]\n",
    "\n",
    "                elif k == 'semantic_scholar':\n",
    "                    query = path.split('\\\\')[-1].split('_')[-1].split('.')[0]\n",
    "                else:\n",
    "                    pass\n",
    "                print('%s 11111 has %d instances.' % (k, df.shape[0]))\n",
    "                df = df[:wanted_size]\n",
    "                df['query'] = query\n",
    "\n",
    "                df['database'] = k\n",
    "                new_cols = list(set(wanted_fields).difference(list(df.columns)))\n",
    "                for col in new_cols:\n",
    "                    df[col] = ''\n",
    "                df = df[wanted_fields]\n",
    "                all_data.append(df)\n",
    "                print('%s 2222 has %d instances.' % (k, df.shape[0]))\n",
    "\n",
    "    all_dfs = pd.concat(all_data)\n",
    "    print('\\nDone! Merged df has %s examples, and %s fields' % (str(all_dfs.shape[0]), str(all_dfs.shape[1])))\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_p = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\"\n",
    "    wanted_fields = ['title', 'authors', 'venue', 'year', 'citationCount', 'fieldsOfStudy', 'abstract', 'doi', 'query',\n",
    "                     'database']\n",
    "    merged_all_data = merge_cross_DBs(root_p, wanted_fields)\n",
    "\n",
    "    out_p = root_p + '\\merged_all_data'\n",
    "    try:\n",
    "        os.makedirs(out_p, exist_ok=True)\n",
    "    except OSError as error:\n",
    "        print('Directory cannot be created!')\n",
    "    with open(out_p + '\\merged_all_data.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        merged_all_data.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7074b",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "- Stardardize datatypes\n",
    "- Normalize(lowercase, etc)\n",
    "- Deduplicate\n",
    "- write to a cleaned csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "686df3f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17092/3211379058.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_p = \n",
    "with open(out_p + '\\merged_all_data.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    all_df.to_csv(f)\n",
    "all_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e59b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHO competency framework for health authoritie...</td>\n",
       "      <td>Rubinelli S,Purnat TD,Wihelm E,Traicoff D,Nama...</td>\n",
       "      <td>Human Resources for Health</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.1186/s12960-022-00733-0</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A cross-sectional study of factors associated ...</td>\n",
       "      <td>Yeager S,Abramovitz D,Harvey-Vera AY,Vera CF,A...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.1186/s12889-022-13273-y</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mapping state-sponsored information operations...</td>\n",
       "      <td>Uyheng J,Cruickshank IJ,Carley KM</td>\n",
       "      <td>EPJ Data Science</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.1140/epjds/s13688-022-00338-6</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A digital media literacy intervention for olde...</td>\n",
       "      <td>Moore RC,Hancock JT</td>\n",
       "      <td>Scientific Reports</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.1038/s41598-022-08437-0</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lateral reading and monetary incentives to spo...</td>\n",
       "      <td>Panizza F,Ronzani P,Martini C,Mattavelli S,Mor...</td>\n",
       "      <td>Scientific Reports</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.1038/s41598-022-09168-y</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>Yolk sac macrophage progenitors traffic to the...</td>\n",
       "      <td>Stremmel, C; Schuchert, R; Wagner, F; Thaler, ...</td>\n",
       "      <td>NATURE PUBLISHING GROUP</td>\n",
       "      <td>2018.0</td>\n",
       "      <td></td>\n",
       "      <td>Science &amp; Technology - Other Topics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41467-018-06065-9</td>\n",
       "      <td>wos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>Nutrient exchange in arbuscular mycorrhizal sy...</td>\n",
       "      <td>Dreyer, I; Spitz, O; Kanonenberg, K; Montag, K...</td>\n",
       "      <td>WILEY</td>\n",
       "      <td>2019.0</td>\n",
       "      <td></td>\n",
       "      <td>Plant Sciences</td>\n",
       "      <td>To obtain insights into the dynamics of nutrie...</td>\n",
       "      <td>10.1111/nph.15646</td>\n",
       "      <td>wos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>A Mobile, Multichannel, UWB Radar for Potentia...</td>\n",
       "      <td>Rodriguez-Morales, F; Braaten, D; Mai, HT; Pad...</td>\n",
       "      <td>IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC</td>\n",
       "      <td>2020.0</td>\n",
       "      <td></td>\n",
       "      <td>Engineering; Physical Geography; Remote Sensin...</td>\n",
       "      <td>We developed a high-performance, multichannel,...</td>\n",
       "      <td>10.1109/JSTARS.2020.3016287</td>\n",
       "      <td>wos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>Two populations of self-maintaining monocyte-i...</td>\n",
       "      <td>Wang, M; Yang, YL; Cansever, D; Wang, YM; Kant...</td>\n",
       "      <td>NATL ACAD SCIENCES</td>\n",
       "      <td>2021.0</td>\n",
       "      <td></td>\n",
       "      <td>Science &amp; Technology - Other Topics</td>\n",
       "      <td>Macrophages are the principal immune cells of ...</td>\n",
       "      <td>10.1073/pnas.2013686117</td>\n",
       "      <td>wos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>Modulation of cellular S1P levels with a novel...</td>\n",
       "      <td>Schnute, ME; McReynolds, MD; Kasten, T; Yates,...</td>\n",
       "      <td>PORTLAND PRESS LTD</td>\n",
       "      <td>2012.0</td>\n",
       "      <td></td>\n",
       "      <td>Biochemistry &amp; Molecular Biology</td>\n",
       "      <td>SphK (sphingosine kinase) is the major source ...</td>\n",
       "      <td>10.1042/BJ20111929</td>\n",
       "      <td>wos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31493 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     WHO competency framework for health authoritie...   \n",
       "1     A cross-sectional study of factors associated ...   \n",
       "2     Mapping state-sponsored information operations...   \n",
       "3     A digital media literacy intervention for olde...   \n",
       "4     Lateral reading and monetary incentives to spo...   \n",
       "...                                                 ...   \n",
       "1693  Yolk sac macrophage progenitors traffic to the...   \n",
       "1694  Nutrient exchange in arbuscular mycorrhizal sy...   \n",
       "1695  A Mobile, Multichannel, UWB Radar for Potentia...   \n",
       "1696  Two populations of self-maintaining monocyte-i...   \n",
       "1697  Modulation of cellular S1P levels with a novel...   \n",
       "\n",
       "                                                authors  \\\n",
       "0     Rubinelli S,Purnat TD,Wihelm E,Traicoff D,Nama...   \n",
       "1     Yeager S,Abramovitz D,Harvey-Vera AY,Vera CF,A...   \n",
       "2                     Uyheng J,Cruickshank IJ,Carley KM   \n",
       "3                                   Moore RC,Hancock JT   \n",
       "4     Panizza F,Ronzani P,Martini C,Mattavelli S,Mor...   \n",
       "...                                                 ...   \n",
       "1693  Stremmel, C; Schuchert, R; Wagner, F; Thaler, ...   \n",
       "1694  Dreyer, I; Spitz, O; Kanonenberg, K; Montag, K...   \n",
       "1695  Rodriguez-Morales, F; Braaten, D; Mai, HT; Pad...   \n",
       "1696  Wang, M; Yang, YL; Cansever, D; Wang, YM; Kant...   \n",
       "1697  Schnute, ME; McReynolds, MD; Kasten, T; Yates,...   \n",
       "\n",
       "                                               venue    year citationCount  \\\n",
       "0                         Human Resources for Health  2022.0           NaN   \n",
       "1                                  BMC Public Health  2022.0           NaN   \n",
       "2                                   EPJ Data Science  2022.0           NaN   \n",
       "3                                 Scientific Reports  2022.0           NaN   \n",
       "4                                 Scientific Reports  2022.0           NaN   \n",
       "...                                              ...     ...           ...   \n",
       "1693                         NATURE PUBLISHING GROUP  2018.0                 \n",
       "1694                                           WILEY  2019.0                 \n",
       "1695  IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC  2020.0                 \n",
       "1696                              NATL ACAD SCIENCES  2021.0                 \n",
       "1697                              PORTLAND PRESS LTD  2012.0                 \n",
       "\n",
       "                                          fieldsOfStudy  \\\n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                 ...   \n",
       "1693                Science & Technology - Other Topics   \n",
       "1694                                     Plant Sciences   \n",
       "1695  Engineering; Physical Geography; Remote Sensin...   \n",
       "1696                Science & Technology - Other Topics   \n",
       "1697                   Biochemistry & Molecular Biology   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                 ...   \n",
       "1693                                                NaN   \n",
       "1694  To obtain insights into the dynamics of nutrie...   \n",
       "1695  We developed a high-performance, multichannel,...   \n",
       "1696  Macrophages are the principal immune cells of ...   \n",
       "1697  SphK (sphingosine kinase) is the major source ...   \n",
       "\n",
       "                                   doi database  \n",
       "0           10.1186/s12960-022-00733-0   scopus  \n",
       "1           10.1186/s12889-022-13273-y   scopus  \n",
       "2     10.1140/epjds/s13688-022-00338-6   scopus  \n",
       "3           10.1038/s41598-022-08437-0   scopus  \n",
       "4           10.1038/s41598-022-09168-y   scopus  \n",
       "...                                ...      ...  \n",
       "1693        10.1038/s41467-018-06065-9      wos  \n",
       "1694                 10.1111/nph.15646      wos  \n",
       "1695       10.1109/JSTARS.2020.3016287      wos  \n",
       "1696           10.1073/pnas.2013686117      wos  \n",
       "1697                10.1042/BJ20111929      wos  \n",
       "\n",
       "[31493 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = all_df.copy()\n",
    "df_copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab23eb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title              0\n",
       "authors            0\n",
       "venue              0\n",
       "year              44\n",
       "citationCount      0\n",
       "fieldsOfStudy      0\n",
       "abstract         215\n",
       "doi              278\n",
       "database           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ddb284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             object\n",
       "authors           object\n",
       "venue             object\n",
       "year             float64\n",
       "citationCount     object\n",
       "fieldsOfStudy     object\n",
       "abstract          object\n",
       "doi               object\n",
       "database          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd6d5b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\n",
      "venue\n",
      "doi\n"
     ]
    }
   ],
   "source": [
    "def lowercase(x):\n",
    "    return x.lower()\n",
    "fields_to_dedup = ['authors', 'venue', 'doi']\n",
    "for field in fields_to_dedup:\n",
    "    print(field)\n",
    "    df_copy[field] = df_copy[field].apply(lambda x: lowercase(x) if type(x)==str else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a5a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41773080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicates using 'title',  'authors', 'venue', 'year':  3363\n",
      "number of duplicates using 'title',  'authors', 'venue':  3381\n",
      "number of duplicates using 'title',  'authors', 'year':  3384\n",
      "number of duplicates using 'title',  'year':  6223\n",
      "number of duplicates using 'title':  6842\n",
      "number of duplicates using 'title', 'doi':  5351\n"
     ]
    }
   ],
   "source": [
    "# Observe the number of duplicates.\n",
    "print(\"number of duplicates using 'title',  'authors', 'venue', 'year': \", df_copy[df_copy.duplicated(subset = ['title',  'authors', 'venue','year'])].shape[0])\n",
    "print(\"number of duplicates using 'title',  'authors', 'venue': \", df_copy[df_copy.duplicated(subset = ['title',  'authors', 'venue'])].shape[0])\n",
    "print(\"number of duplicates using 'title',  'authors', 'year': \", df_copy[df_copy.duplicated(subset = ['title',  'authors', 'year'])].shape[0])\n",
    "print(\"number of duplicates using 'title',  'year': \", df_copy[df_copy.duplicated(subset = ['title',  'year'])].shape[0])\n",
    "print(\"number of duplicates using 'title': \", df_copy[df_copy.duplicated(subset = ['title'])].shape[0])\n",
    "print(\"number of duplicates using 'title', 'doi': \", df_copy[df_copy.duplicated(subset = ['title', 'doi'])].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe508463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate\n",
    "df_deduplicated = df_copy[df_copy.duplicated(subset = ['title',  'year'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
