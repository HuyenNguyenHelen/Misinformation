{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060188b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a34b84",
   "metadata": {},
   "source": [
    "# Merging metadata files\n",
    "- Merging metadata files collected from four databases\n",
    "- Output: a single df with the wanted fields: 'title', 'authors', 'venue', 'year', 'citationCount', 'fieldsOfStudy', 'abstract', 'doi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72b9fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disinformation\n",
      "fake+news\n",
      "spam\n",
      "troll\n",
      "all_dfs.shape (4100, 16)\n",
      "misinformation\n",
      "rumor\n",
      "dfs.shape (2119, 16)\n",
      "the SD concatenated data has 6219 examples, and 16 columns.\n",
      "Index(['Item type', 'Authors', 'Title', 'Journal', 'Publication year',\n",
      "       'Volume', 'Issue', 'Pages', 'Date published', 'ISSN', 'URLs', 'DOI',\n",
      "       'Abstract', 'Keywords', 'Notes', 'query'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Science Direct\n",
    "\n",
    "# Collected Files exist in the folders:\n",
    "# raw_ScienceDirect____ ScienceDirect___________ SD Disinfo: 10 files\n",
    "#                   |                  |_____ SD fake news: 10 files\n",
    "#                   |                  |_____ SD Spam: 10 files\n",
    "#                   |                  |_____ SD troll: 10 files\n",
    "#                   |__ ScienceDirect______ metadata_ScDr_misinformation.csv\n",
    "#                                      |__ metadata_ScDr_rumor.csv\n",
    "\n",
    "\n",
    "def merge_SD_multi(root_p, wanted_fields, wanted_size):\n",
    "    query_name = {\n",
    "        'SD Disinfo': 'disinformation',\n",
    "        'SD fake news': 'fake+news',\n",
    "        'SD Spam' : 'spam',\n",
    "        'SD troll': 'troll'\n",
    "    }\n",
    "    all_data = []\n",
    "    for fold in os.listdir(root_p): \n",
    "        print(query_name[fold])\n",
    "        data = []\n",
    "        for file_p in glob.glob('%s\\%s\\*.csv'%(root_p, fold)):\n",
    "#             print(file_p)\n",
    "            try:\n",
    "                with open(file_p, 'r') as f:\n",
    "                    df = pd.read_csv(f)\n",
    "            except:\n",
    "                with open(file_p, 'r', encoding = 'utf-8') as f:\n",
    "                    df = pd.read_csv(f)        \n",
    "            data.append(df)\n",
    "            dfs = pd.concat(data)\n",
    "            dfs['query'] = query_name[fold]\n",
    "            \n",
    "            if wanted_size is not None:\n",
    "                wanted_size = wanted_size\n",
    "            else:\n",
    "                wanted_size = len(dfs)\n",
    "#             print('df.shape', dfs.shape)        \n",
    "\n",
    "            all_data.append(dfs[wanted_fields][:wanted_size])\n",
    "\n",
    "    all_dfs = pd.concat(all_data)\n",
    "    print('all_dfs.shape', all_dfs.shape)\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "def merge_SD_single(root_p, wanted_fields, wanted_size):\n",
    "    data = []\n",
    "    for file_p in glob.glob('%s\\*.csv'%(root_p)):\n",
    "        query = file_p.split('\\\\')[-1].split('_')[-1].split('.')[0]\n",
    "        print(query)\n",
    "        try:\n",
    "            with open(file_p, 'r') as f:\n",
    "                df = pd.read_csv(f)\n",
    "        except:\n",
    "            with open(file_p, 'r', encoding = 'utf-8') as f:\n",
    "                df = pd.read_csv(f)\n",
    "        df['query'] = query\n",
    "        \n",
    "        if wanted_size is not None:\n",
    "                wanted_size = wanted_size\n",
    "        else:\n",
    "            wanted_size = len(df)\n",
    "        data.append(df[wanted_fields][:wanted_size])\n",
    "    dfs = pd.concat(data)\n",
    "    print('dfs.shape', dfs.shape)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wanted_fields = ['Item type', 'Authors', 'Title', 'Journal', 'Publication year',\n",
    "       'Volume', 'Issue', 'Pages', 'Date published', 'ISSN', 'URLs', 'DOI',\n",
    "       'Abstract', 'Keywords', 'Notes', 'query']\n",
    "    root = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\"\n",
    "    out_p = root + '\\merged_ScienceDirect'\n",
    "        \n",
    "    sd_multi_f = merge_SD_multi(\n",
    "        root + \"\\\\raw_ScienceDirect\\\\ScienceDirect\", wanted_fields, wanted_size = None) \n",
    "    sd_single_f = merge_SD_single(\n",
    "        root + \"\\\\raw_ScienceDirect\\\\Science_direct\", wanted_fields, wanted_size = None)\n",
    "    concat_SD = pd.concat([sd_multi_f, sd_single_f])\n",
    "    print('the SD concatenated data has %d examples, and %d columns.'%(concat_SD.shape[0], concat_SD.shape[1]))\n",
    "    print(concat_SD.columns)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(out_p, exist_ok = True)\n",
    "    except OSError as error:\n",
    "        print('Directory cannot be created!')\n",
    "    with open(out_p + '\\ScienceDirect.csv', 'w', encoding = 'utf-8', newline = '') as f:\n",
    "        concat_SD.to_csv(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4cab439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____scopus_____\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "scopus 11111 has 2000 instances.\n",
      "scopus 2222 has 1000 instances.\n",
      "wanted_size: 897\n",
      "scopus 11111 has 897 instances.\n",
      "scopus 2222 has 897 instances.\n",
      "\n",
      "_____semantic_scholar_____\n",
      "wanted_size: 1000\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "wanted_size: 990\n",
      "semantic_scholar 11111 has 990 instances.\n",
      "semantic_scholar 2222 has 990 instances.\n",
      "\n",
      "_____wos_____\n",
      "wanted_size: 1000\n",
      "wos 11111 has 1813 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 3584 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 6313 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 3755 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 2493 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "wanted_size: 1000\n",
      "wos 11111 has 1698 instances.\n",
      "wos 2222 has 1000 instances.\n",
      "\n",
      "_____science_direct_____\n",
      "Index(['Unnamed: 0', 'Item type', 'Authors', 'Title', 'Journal',\n",
      "       'Publication year', 'Volume', 'Issue', 'Pages', 'Date published',\n",
      "       'ISSN', 'URLs', 'DOI', 'Abstract', 'Keywords', 'Notes', 'query'],\n",
      "      dtype='object')\n",
      "\n",
      "Done! Merged df has 24056 examples, and 10 fields\n"
     ]
    }
   ],
   "source": [
    "def merge_cross_DBs(root_p, wanted_fields, wanted_size=1000):\n",
    "    fold_names = {\n",
    "        'scopus': 'Scopus csv',\n",
    "        'semantic_scholar': \"Semantic_scholar\",\n",
    "        'wos': \"WoS_Metadata\",\n",
    "        'science_direct': 'merged_ScienceDirect'\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "    for k, v in fold_names.items():\n",
    "        print('\\n_____%s_____' % k)\n",
    "        fold_p = root_p + \"\\\\\" + v\n",
    "        file_ps = [f for f in glob.glob(fold_p + '\\*.csv')]\n",
    "        if k == 'science_direct':\n",
    "            if len(file_ps) == 1:\n",
    "                with open(file_ps[0], 'r', encoding='utf-8') as f:\n",
    "                    df = pd.read_csv(f)\n",
    "            else:\n",
    "                print('more than one file in the directory')\n",
    "            print(df.columns)\n",
    "            df.columns = ['Unnamed: 0', 'Item type', 'authors', 'title', 'venue', 'year', 'Volume', 'Issue', 'Pages',\n",
    "                          'Date published', \n",
    "                          'ISSN', 'URLs', 'doi', 'abstract', 'Keywords', 'Notes', 'query']\n",
    "            df['database'] = k\n",
    "            new_cols = list(set(wanted_fields).difference(list(df.columns)))\n",
    "            for col in new_cols:\n",
    "                df[col] = ''\n",
    "            df = df[wanted_fields]\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            for path in file_ps:\n",
    "                try:\n",
    "                    with open(path, 'r') as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                except:\n",
    "                    with open(path, 'r', encoding='utf-8') as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                if wanted_size < len(df):\n",
    "                    wanted_size = 1000\n",
    "                else:\n",
    "                    wanted_size = len(df)\n",
    "                print('wanted_size: %d'%wanted_size)\n",
    "                \n",
    "                if k == 'scopus':\n",
    "                    df.columns = ['Item type', 'authors', 'title', 'venue', 'year', 'Volume',\n",
    "                                  'Issue', 'Pages', 'Date published', 'URLs', 'doi', 'Notes', 'citationCount']\n",
    "\n",
    "                    query = path.split('\\\\')[-1].split(' ')[-1].split('.')[0]\n",
    "                elif k == 'wos':\n",
    "                    df.columns = ['Publication Type', 'authors', 'Book Authors', 'Book Editors',\n",
    "                                  'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
    "                                  'Group Authors', 'title', 'Source Title', 'Book Series Title',\n",
    "                                  'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
    "                                  'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
    "                                  'Conference Host', 'Author Keywords', 'Keywords Plus', 'abstract',\n",
    "                                  'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
    "                                  'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
    "                                  'Funding Text', 'Cited References', 'Cited Reference Count',\n",
    "                                  'citationCount, WoS Core', 'Times Cited, All Databases',\n",
    "                                  '180 Day Usage Count', 'Since 2013 Usage Count', 'venue',\n",
    "                                  'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
    "                                  'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
    "                                  'year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
    "                                  'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
    "                                  'Article Number', 'doi', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
    "                                  'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
    "                                  'fieldsOfStudy', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
    "                                  'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
    "                                  'UT (Unique WOS ID)', 'Web of Science Record']\n",
    "                    query = path.split('\\\\')[-1].split('_')[-1].split('.')[0]\n",
    "\n",
    "                elif k == 'semantic_scholar':\n",
    "                    query = path.split('\\\\')[-1].split('_')[-1].split('.')[0]\n",
    "                else:\n",
    "                    pass\n",
    "                print('%s 11111 has %d instances.' % (k, df.shape[0]))\n",
    "                df = df[:wanted_size]\n",
    "                df['query'] = query\n",
    "\n",
    "                df['database'] = k\n",
    "                new_cols = list(set(wanted_fields).difference(list(df.columns)))\n",
    "                for col in new_cols:\n",
    "                    df[col] = ''\n",
    "                df = df[wanted_fields]\n",
    "                all_data.append(df)\n",
    "                print('%s 2222 has %d instances.' % (k, df.shape[0]))\n",
    "\n",
    "    all_dfs = pd.concat(all_data)\n",
    "    print('\\nDone! Merged df has %s examples, and %s fields' % (str(all_dfs.shape[0]), str(all_dfs.shape[1])))\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_p = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\"\n",
    "    wanted_fields = ['title', 'authors', 'venue', 'year', 'citationCount', 'fieldsOfStudy', 'abstract', 'doi', 'query',\n",
    "                     'database']\n",
    "    merged_all_data = merge_cross_DBs(root_p, wanted_fields)\n",
    "\n",
    "    out_p = root_p + '\\merged_all_data'\n",
    "    try:\n",
    "        os.makedirs(out_p, exist_ok=True)\n",
    "    except OSError as error:\n",
    "        print('Directory cannot be created!')\n",
    "    with open(out_p + '\\merged_all_data.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        merged_all_data.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629cf53",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "- Stardardize datatypes\n",
    "- Normalize(lowercase, etc)\n",
    "- Deduplicate\n",
    "- write to a cleaned csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67fe86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>query</th>\n",
       "      <th>database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>WHO competency framework for health authoritie...</td>\n",
       "      <td>Rubinelli S,Purnat TD,Wihelm E,Traicoff D,Nama...</td>\n",
       "      <td>Human Resources for Health</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12960-022-00733-0</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A cross-sectional study of factors associated ...</td>\n",
       "      <td>Yeager S,Abramovitz D,Harvey-Vera AY,Vera CF,A...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12889-022-13273-y</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mapping state-sponsored information operations...</td>\n",
       "      <td>Uyheng J,Cruickshank IJ,Carley KM</td>\n",
       "      <td>EPJ Data Science</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1140/epjds/s13688-022-00338-6</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  WHO competency framework for health authoritie...   \n",
       "1           1  A cross-sectional study of factors associated ...   \n",
       "2           2  Mapping state-sponsored information operations...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Rubinelli S,Purnat TD,Wihelm E,Traicoff D,Nama...   \n",
       "1  Yeager S,Abramovitz D,Harvey-Vera AY,Vera CF,A...   \n",
       "2                  Uyheng J,Cruickshank IJ,Carley KM   \n",
       "\n",
       "                        venue  year citationCount fieldsOfStudy abstract  \\\n",
       "0  Human Resources for Health  2022           NaN           NaN      NaN   \n",
       "1           BMC Public Health  2022           NaN           NaN      NaN   \n",
       "2            EPJ Data Science  2022           NaN           NaN      NaN   \n",
       "\n",
       "                                doi           query database  \n",
       "0        10.1186/s12960-022-00733-0  disinformation   scopus  \n",
       "1        10.1186/s12889-022-13273-y  disinformation   scopus  \n",
       "2  10.1140/epjds/s13688-022-00338-6  disinformation   scopus  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_p = r\"C:\\Users\\huyen\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\\merged_all_data\\merged_all_data.csv\"\n",
    "with open(data_p, 'r', encoding='utf-8', newline='') as f:\n",
    "    all_df = pd.read_csv(f)\n",
    "all_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4430f74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>query</th>\n",
       "      <th>database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>WHO competency framework for health authoritie...</td>\n",
       "      <td>Rubinelli S,Purnat TD,Wihelm E,Traicoff D,Nama...</td>\n",
       "      <td>Human Resources for Health</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12960-022-00733-0</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A cross-sectional study of factors associated ...</td>\n",
       "      <td>Yeager S,Abramovitz D,Harvey-Vera AY,Vera CF,A...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12889-022-13273-y</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mapping state-sponsored information operations...</td>\n",
       "      <td>Uyheng J,Cruickshank IJ,Carley KM</td>\n",
       "      <td>EPJ Data Science</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1140/epjds/s13688-022-00338-6</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A digital media literacy intervention for olde...</td>\n",
       "      <td>Moore RC,Hancock JT</td>\n",
       "      <td>Scientific Reports</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41598-022-08437-0</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lateral reading and monetary incentives to spo...</td>\n",
       "      <td>Panizza F,Ronzani P,Martini C,Mattavelli S,Mor...</td>\n",
       "      <td>Scientific Reports</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41598-022-09168-y</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24051</th>\n",
       "      <td>6214</td>\n",
       "      <td>Effective strategies for responding to rumors ...</td>\n",
       "      <td>Paek HJ,Hove T</td>\n",
       "      <td>Public Relations Review</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This experimental study explores how governmen...</td>\n",
       "      <td>10.1016/j.pubrev.2019.02.006</td>\n",
       "      <td>rumor</td>\n",
       "      <td>science_direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24052</th>\n",
       "      <td>6215</td>\n",
       "      <td>Factors affecting individual online rumor shar...</td>\n",
       "      <td>Luo P,Wang C,Guo F,Luo L</td>\n",
       "      <td>Computers in Human Behavior</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>With the outbreak of COVID-19, online sharing ...</td>\n",
       "      <td>10.1016/j.chb.2021.106968</td>\n",
       "      <td>rumor</td>\n",
       "      <td>science_direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24053</th>\n",
       "      <td>6216</td>\n",
       "      <td>Stability and Hopf bifurcation analysis of mul...</td>\n",
       "      <td>Wang J,Jiang H,Hu C,Yu Z,Li J</td>\n",
       "      <td>Chaos, Solitons &amp; Fractals</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stability and hopf bifurcation of the mult...</td>\n",
       "      <td>10.1016/j.chaos.2021.111464</td>\n",
       "      <td>rumor</td>\n",
       "      <td>science_direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24054</th>\n",
       "      <td>6217</td>\n",
       "      <td>Soft rumor control in social networks: Modelin...</td>\n",
       "      <td>Askarizadeh M,Tork Ladani B</td>\n",
       "      <td>Engineering Applications of Artificial Intelli...</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nowadays, social networks become ubiquitous pl...</td>\n",
       "      <td>10.1016/j.engappai.2021.104198</td>\n",
       "      <td>rumor</td>\n",
       "      <td>science_direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24055</th>\n",
       "      <td>6218</td>\n",
       "      <td>Genetic algorithm based rumor mitigation in on...</td>\n",
       "      <td>Parimi P,Rout RR</td>\n",
       "      <td>Information Processing &amp; Management</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Immense use of social media platforms results ...</td>\n",
       "      <td>10.1016/j.ipm.2021.102669</td>\n",
       "      <td>rumor</td>\n",
       "      <td>science_direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24056 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              title  \\\n",
       "0               0  WHO competency framework for health authoritie...   \n",
       "1               1  A cross-sectional study of factors associated ...   \n",
       "2               2  Mapping state-sponsored information operations...   \n",
       "3               3  A digital media literacy intervention for olde...   \n",
       "4               4  Lateral reading and monetary incentives to spo...   \n",
       "...           ...                                                ...   \n",
       "24051        6214  Effective strategies for responding to rumors ...   \n",
       "24052        6215  Factors affecting individual online rumor shar...   \n",
       "24053        6216  Stability and Hopf bifurcation analysis of mul...   \n",
       "24054        6217  Soft rumor control in social networks: Modelin...   \n",
       "24055        6218  Genetic algorithm based rumor mitigation in on...   \n",
       "\n",
       "                                                 authors  \\\n",
       "0      Rubinelli S,Purnat TD,Wihelm E,Traicoff D,Nama...   \n",
       "1      Yeager S,Abramovitz D,Harvey-Vera AY,Vera CF,A...   \n",
       "2                      Uyheng J,Cruickshank IJ,Carley KM   \n",
       "3                                    Moore RC,Hancock JT   \n",
       "4      Panizza F,Ronzani P,Martini C,Mattavelli S,Mor...   \n",
       "...                                                  ...   \n",
       "24051                                     Paek HJ,Hove T   \n",
       "24052                           Luo P,Wang C,Guo F,Luo L   \n",
       "24053                      Wang J,Jiang H,Hu C,Yu Z,Li J   \n",
       "24054                        Askarizadeh M,Tork Ladani B   \n",
       "24055                                   Parimi P,Rout RR   \n",
       "\n",
       "                                                   venue  year citationCount  \\\n",
       "0                             Human Resources for Health  2022           NaN   \n",
       "1                                      BMC Public Health  2022           NaN   \n",
       "2                                       EPJ Data Science  2022           NaN   \n",
       "3                                     Scientific Reports  2022           NaN   \n",
       "4                                     Scientific Reports  2022           NaN   \n",
       "...                                                  ...   ...           ...   \n",
       "24051                            Public Relations Review  2019           NaN   \n",
       "24052                        Computers in Human Behavior  2021           NaN   \n",
       "24053                         Chaos, Solitons & Fractals  2021           NaN   \n",
       "24054  Engineering Applications of Artificial Intelli...  2021           NaN   \n",
       "24055                Information Processing & Management  2021           NaN   \n",
       "\n",
       "      fieldsOfStudy                                           abstract  \\\n",
       "0               NaN                                                NaN   \n",
       "1               NaN                                                NaN   \n",
       "2               NaN                                                NaN   \n",
       "3               NaN                                                NaN   \n",
       "4               NaN                                                NaN   \n",
       "...             ...                                                ...   \n",
       "24051           NaN  This experimental study explores how governmen...   \n",
       "24052           NaN  With the outbreak of COVID-19, online sharing ...   \n",
       "24053           NaN  The stability and hopf bifurcation of the mult...   \n",
       "24054           NaN  Nowadays, social networks become ubiquitous pl...   \n",
       "24055           NaN  Immense use of social media platforms results ...   \n",
       "\n",
       "                                    doi           query        database  \n",
       "0            10.1186/s12960-022-00733-0  disinformation          scopus  \n",
       "1            10.1186/s12889-022-13273-y  disinformation          scopus  \n",
       "2      10.1140/epjds/s13688-022-00338-6  disinformation          scopus  \n",
       "3            10.1038/s41598-022-08437-0  disinformation          scopus  \n",
       "4            10.1038/s41598-022-09168-y  disinformation          scopus  \n",
       "...                                 ...             ...             ...  \n",
       "24051      10.1016/j.pubrev.2019.02.006           rumor  science_direct  \n",
       "24052         10.1016/j.chb.2021.106968           rumor  science_direct  \n",
       "24053       10.1016/j.chaos.2021.111464           rumor  science_direct  \n",
       "24054    10.1016/j.engappai.2021.104198           rumor  science_direct  \n",
       "24055         10.1016/j.ipm.2021.102669           rumor  science_direct  \n",
       "\n",
       "[24056 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = all_df.copy()\n",
    "df_copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ba3049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           0\n",
       "title                0\n",
       "authors            232\n",
       "venue             3370\n",
       "year               358\n",
       "citationCount    17977\n",
       "fieldsOfStudy    12321\n",
       "abstract          7893\n",
       "doi               7149\n",
       "query                0\n",
       "database             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0bf6c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        int64\n",
       "title            object\n",
       "authors          object\n",
       "venue            object\n",
       "year             object\n",
       "citationCount    object\n",
       "fieldsOfStudy    object\n",
       "abstract         object\n",
       "doi              object\n",
       "query            object\n",
       "database         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26279a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "authors\n",
      "venue\n",
      "doi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>query</th>\n",
       "      <th>database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>who competency framework for health authoritie...</td>\n",
       "      <td>rubinelli s,purnat td,wihelm e,traicoff d,nama...</td>\n",
       "      <td>human resources for health</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12960-022-00733-0</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a cross-sectional study of factors associated ...</td>\n",
       "      <td>yeager s,abramovitz d,harvey-vera ay,vera cf,a...</td>\n",
       "      <td>bmc public health</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1186/s12889-022-13273-y</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mapping state-sponsored information operations...</td>\n",
       "      <td>uyheng j,cruickshank ij,carley km</td>\n",
       "      <td>epj data science</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1140/epjds/s13688-022-00338-6</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a digital media literacy intervention for olde...</td>\n",
       "      <td>moore rc,hancock jt</td>\n",
       "      <td>scientific reports</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41598-022-08437-0</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lateral reading and monetary incentives to spo...</td>\n",
       "      <td>panizza f,ronzani p,martini c,mattavelli s,mor...</td>\n",
       "      <td>scientific reports</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41598-022-09168-y</td>\n",
       "      <td>disinformation</td>\n",
       "      <td>scopus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  who competency framework for health authoritie...   \n",
       "1           1  a cross-sectional study of factors associated ...   \n",
       "2           2  mapping state-sponsored information operations...   \n",
       "3           3  a digital media literacy intervention for olde...   \n",
       "4           4  lateral reading and monetary incentives to spo...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  rubinelli s,purnat td,wihelm e,traicoff d,nama...   \n",
       "1  yeager s,abramovitz d,harvey-vera ay,vera cf,a...   \n",
       "2                  uyheng j,cruickshank ij,carley km   \n",
       "3                                moore rc,hancock jt   \n",
       "4  panizza f,ronzani p,martini c,mattavelli s,mor...   \n",
       "\n",
       "                        venue  year citationCount fieldsOfStudy abstract  \\\n",
       "0  human resources for health  2022           NaN           NaN      NaN   \n",
       "1           bmc public health  2022           NaN           NaN      NaN   \n",
       "2            epj data science  2022           NaN           NaN      NaN   \n",
       "3          scientific reports  2022           NaN           NaN      NaN   \n",
       "4          scientific reports  2022           NaN           NaN      NaN   \n",
       "\n",
       "                                doi           query database  \n",
       "0        10.1186/s12960-022-00733-0  disinformation   scopus  \n",
       "1        10.1186/s12889-022-13273-y  disinformation   scopus  \n",
       "2  10.1140/epjds/s13688-022-00338-6  disinformation   scopus  \n",
       "3        10.1038/s41598-022-08437-0  disinformation   scopus  \n",
       "4        10.1038/s41598-022-09168-y  disinformation   scopus  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase(x):\n",
    "    return x.lower()\n",
    "fields_to_dedup = ['title', 'authors', 'venue', 'doi']\n",
    "for field in fields_to_dedup:\n",
    "    print(field)\n",
    "    df_copy[field] = df_copy[field].apply(lambda x: lowercase(x) if type(x)==str else x)\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a091a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicates using 'title',  'authors', 'venue', 'year':  5413\n",
      "number of duplicates using 'title',  'authors', 'venue':  5590\n",
      "number of duplicates using 'title',  'authors', 'year':  5491\n",
      "number of duplicates using 'title',  'year':  6284\n",
      "number of duplicates using 'title', 'doi':  7264\n",
      "number of duplicates using 'title':  8452\n",
      "number of duplicates using 'doi':  16888\n"
     ]
    }
   ],
   "source": [
    "# Observe the number of duplicates.\n",
    "print(\"number of duplicates using 'title',  'authors', 'venue', 'year': \", df_copy[df_copy.duplicated(subset = ['title',  'authors', 'venue','year'])].shape[0])\n",
    "print(\"number of duplicates using 'title',  'authors', 'venue': \", df_copy[df_copy.duplicated(subset = ['title',  'authors', 'venue'])].shape[0])\n",
    "print(\"number of duplicates using 'title',  'authors', 'year': \", df_copy[df_copy.duplicated(subset = ['title',  'authors', 'year'])].shape[0])\n",
    "print(\"number of duplicates using 'title',  'year': \", df_copy[df_copy.duplicated(subset = ['title',  'year'])].shape[0])\n",
    "print(\"number of duplicates using 'title', 'doi': \", df_copy[df_copy.duplicated(subset = ['title', 'doi'])].shape[0])\n",
    "print(\"number of duplicates using 'title': \", df_copy[df_copy.duplicated(subset = ['title'])].shape[0])\n",
    "print(\"number of duplicates using 'doi': \", df_copy[df_copy['doi'].str.len()>3].duplicated(subset = ['doi']).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "781f0aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9949"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_copy[df_copy['doi'].str.len()>3].drop_duplicates(subset = ['doi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ec9929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11631"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deduplicate\n",
    "df_deduplicated = df_copy[df_copy['doi'].str.len()>3].drop_duplicates(subset = ['title',  'year'])\n",
    "len(df_deduplicated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
