{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d648ad",
   "metadata": {},
   "source": [
    "# Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1673bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'authors', 'venue', 'year', 'citationCount',\n",
      "       'fieldsOfStudy', 'abstract', 'doi', 'query', 'database',\n",
      "       'publication_type'],\n",
      "      dtype='object')\n",
      "[('social media', 9.924303411959038e-07), ('fake news', 1.3047250484093075e-06), ('social media platforms', 2.5632053191541136e-06), ('fake news detection', 2.640716614146843e-06), ('online social networks', 3.395765128235662e-06), ('social media use', 4.5968016736445965e-06), ('social media users', 5.411398006995505e-06), ('social networks', 6.61669595737636e-06), ('social media news', 8.450279803841577e-06), ('social', 8.499241242615078e-06), ('information', 8.783742531554572e-06), ('social media information', 9.109590952449133e-06), ('online social media', 9.394470102162478e-06), ('using social media', 9.861601659799538e-06), ('news', 1.0067561475624971e-05), ('media', 1.251527732559419e-05), ('social media networks', 1.2851504092180756e-05), ('misinformation', 1.4061149016683143e-05), ('social media data', 1.4357904744197948e-05), ('social media misinformation', 1.4497503775720887e-05), ('social media rumor', 1.5035576505464512e-05), ('online fake news', 1.5447548406878292e-05), ('public health', 1.686749120318783e-05), ('social media content', 1.7492105918025996e-05), ('study', 1.8544175445586455e-05), ('used social media', 1.9485397226633535e-05), ('rumor spreading model', 1.9977059268223855e-05), ('social network analysis', 2.0858260990477103e-05), ('online social', 2.1789118445120297e-05), ('fake', 2.293005370776921e-05), ('health information', 2.3527313254290187e-05), ('detect fake news', 2.563220200529922e-05), ('news media', 2.6507024485196167e-05), ('social media posts', 2.689679174620373e-05), ('results', 2.9958481541827125e-05), ('health', 3.0394881722687434e-05), ('rumor propagation model', 3.1127848040802724e-05), ('spam', 3.2048922867267484e-05), ('model', 3.3100429988677664e-05), ('new', 3.318566560483787e-05), ('news detection', 3.358723551693743e-05), ('social networks social', 3.5021053730882586e-05), ('spam detection', 3.543759307978592e-05), ('news social media', 3.59136891663267e-05), ('fake news social', 3.65736632308584e-05), ('social media social', 3.7765470658262826e-05), ('public health information', 3.8780924001039674e-05), ('rumor', 3.94530751434657e-05), ('via social media', 3.9732779948503445e-05), ('online health information', 4.020433436238659e-05), ('health misinformation', 4.024029892208074e-05), ('social media using', 4.0542140156953655e-05), ('results show', 4.5147702391172216e-05), ('media social media', 4.551956969862823e-05), ('fake news online', 4.80590394880658e-05), ('public', 4.807196689869554e-05), ('media platforms', 4.890237103269411e-05), ('detecting fake news', 4.9519831101230186e-05), ('false information', 4.968718902340935e-05), ('online', 5.0604697688844115e-05), ('data', 5.114391503688747e-05), ('analysis', 5.190124189913893e-05), ('fake news spreading', 5.2147640774124185e-05), ('spreading fake news', 5.214764077412419e-05), ('using', 5.2664550246088076e-05), ('different social media', 5.348030270389168e-05), ('research', 5.409975631299479e-05), ('among social media', 5.451533330978138e-05), ('fake news research', 5.560004621429917e-05), ('rumors', 5.56219143601562e-05), ('fake news spread', 5.5789569459458474e-05), ('new media', 5.613252243923895e-05), ('news fake news', 5.796538622087688e-05), ('social media based', 5.8961400129044786e-05), ('misinformation effect', 5.941451303959957e-05), ('social media analysis', 5.984074131631809e-05), ('social media companies', 6.0436876695988885e-05), ('rumor detection model', 6.060097231495443e-05), ('social media sites', 6.090771071392683e-05), ('rumor detection', 6.120872443810658e-05), ('share fake news', 6.128004618272604e-05), ('vaccine', 6.132541008276274e-05), ('rumor spreading', 6.167735896457248e-05), ('online health misinformation', 6.233220262148384e-05), ('social network information', 6.283168434483707e-05), ('methods', 6.377717382035352e-05), ('social media may', 6.385030675712637e-05), ('fake health news', 6.771414934972221e-05), ('networks', 6.773762648975029e-05), ('online news media', 6.788701303972434e-05), ('news detection using', 6.845091152021114e-05), ('based', 7.040806517404229e-05), ('fake news related', 7.063536389391162e-05), ('network', 7.084802770611638e-05), ('fake news based', 7.145488237961703e-05), ('social media research', 7.34200794672192e-05), ('media use', 7.382957806130184e-05), ('networks social media', 7.389614853003935e-05), ('social networks using', 7.419516952311587e-05), ('social network spam', 7.459679325424827e-05)]\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import yake\n",
    "import nltk\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def yake_extractor(data_list, lemmatizer, stopwords, language = \"en\", max_ngram_size = 3, deduplication_thresold = 0.9, \n",
    "    deduplication_algo = 'seqm', windowSize = 1, numOfKeywords =100):\n",
    "    # preprocess data\n",
    "    lower_data = [data.lower() for data in data_list]\n",
    "    lem_data = [lemmatizer.lemmatize(data) for data in lower_data]\n",
    "    # text = []\n",
    "    # for data in lem_data:\n",
    "    #     text.append(' '.join([w for w in data.split() if w not in stopwords]))\n",
    "    text = ' '.join(lem_data)\n",
    "    # extract keywords\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size,  dedupLim=deduplication_thresold,\n",
    "                            dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None, stopwords=stopwords)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    return keywords\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stopwords = set(STOPWORDS)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    data_path = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\\merged_all_data\\journal+doi+abstract+year+citation+fieldofstudy_dropnull.csv\"\n",
    "    out_path = r'C:\\Users\\hn0139\\Documents\\GitHub\\Misinformation\\Data analysis\\Content analysis\\keyword.csv'\n",
    "    \n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f:    \n",
    "        data = pd.read_csv(f)\n",
    "    print(data.columns)\n",
    "\n",
    "    join_fn = lambda x : ' '.join([x.title, x.abstract])\n",
    "    join_text = data.apply(join_fn, axis=1)\n",
    "    keywords = yake_extractor(data_list = join_text.tolist(), lemmatizer = lemmatizer, stopwords = stopwords)\n",
    "    \n",
    "    # write output to file\n",
    "    f = open(out_path, 'w')\n",
    "    f.writelines('keyword,score\\n')\n",
    "    for (w, s) in keywords: \n",
    "        f = open(out_path, 'a+')\n",
    "        f.write('%s,%f\\n' %(w,s))   \n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22296d",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c39d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 30 ---------\n",
      "Coherence Score:  0.4849451488002597\n",
      "best n_topic: 30 topics. Cohenrence score: {30: 0.4849451488002597} \n",
      "Topics: \n",
      "             0         1            2          3         4          5  \\\n",
      "0         spam      mail       attack     messag     email     system   \n",
      "1       social     media       inform  misinform   twitter    content   \n",
      "2        model   network       spread      rumor    inform     propag   \n",
      "3       rumour    market         food     consum   product      stock   \n",
      "4        coupl       lie      pathway     poison     toler     pseudo   \n",
      "5      patient     medic    treatment    publish    clinic       case   \n",
      "6        cyber     youth   cyberbulli    aggress      self      trait   \n",
      "7       inform   student        studi   research      educ   literaci   \n",
      "8       inform    commun        manag       risk      care     health   \n",
      "9        troll     studi        onlin        use     bulli   behavior   \n",
      "10      review      user     research        use      base       data   \n",
      "11  contracept    random      formula      graph      call    station   \n",
      "12      detect      spam       propos     method      base     featur   \n",
      "13        game     level     interact       echo      book    chamber   \n",
      "14   misinform    effect       memori   particip   correct       fals   \n",
      "15       video     women      adolesc     victim    youtub     gender   \n",
      "16     particl     sourc         mass      speci      size      singl   \n",
      "17       covid    health       pandem      studi    public     inform   \n",
      "18    children    parent         cell  interview     child     famili   \n",
      "19        imag    visual    instagram      panic  advertis        buy   \n",
      "20      energi    sensor       island       rout     flood  christian   \n",
      "21       polit     media        elect    ideolog     right    russian   \n",
      "22        fish    surfac  environment      estim     field      water   \n",
      "23      vaccin     hesit       accept    barrier    commun      uptak   \n",
      "24     perpetr     urban        fatal     taiwan    domest      middl   \n",
      "25        news      fake        media     inform   credibl     social   \n",
      "26      scienc  scientif     knowledg     polici    climat     public   \n",
      "27     comment    signal         blog      emiss  linguist       film   \n",
      "28   disinform    articl       commun     public     digit     govern   \n",
      "29       rumor    social        abort      event    attent     inform   \n",
      "\n",
      "            6             7           8          9  \n",
      "0        page         secur       mobil       rate  \n",
      "1        user      platform       tweet    analysi  \n",
      "2      social         dynam     control       node  \n",
      "3        cost         price       trade     impact  \n",
      "4     scandal          gate     hotspot       beij  \n",
      "5      report        disord      articl    journal  \n",
      "6      person  relationship      answer       dark  \n",
      "7     univers      particip      social       find  \n",
      "8      provid          need         use    respons  \n",
      "9      measur       analysi        data      indic  \n",
      "10     inform         onlin     develop      paper  \n",
      "11       time       nuclear    protocol      accid  \n",
      "12      model         learn         use  algorithm  \n",
      "13     liquid       dietari       micro      smoke  \n",
      "14     inform          test       studi     belief  \n",
      "15     school         score      sexual       year  \n",
      "16    aerosol      concentr      period     observ  \n",
      "17  misinform       prevent       peopl       risk  \n",
      "18     mother        report       studi      trial  \n",
      "19    project        forest     densiti        tag  \n",
      "20      renew       entropi  cryptocurr   protocol  \n",
      "21   campaign        social      speech      onlin  \n",
      "22       area          land       catch     region  \n",
      "23     health         immun     concern     includ  \n",
      "24  diplomaci        opioid       asian      entri  \n",
      "25      share          fact       studi      trust  \n",
      "26       evid        expert      access  scientist  \n",
      "27       coal         humor      annual    biomass  \n",
      "28   research          case       state      polit  \n",
      "29      weibo     microblog       model      refut  \n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def preprocessing (tokenizer, text):\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    \"\"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "     # Remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stopwords]\n",
    "    # Stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens if len(i)>3]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def get_lda_topic (texts, n_topics =[20, 25, 30, 35, 40], n_words = 12):\n",
    "    \"\"\"\n",
    "    texts: a list of token lists\n",
    "    \"\"\"\n",
    "    # Turn our tokenized documents into a id - term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Find the best n_topics\n",
    "    models = {}\n",
    "    topics = {} \n",
    "    coherence = {}\n",
    "    for i in n_topics:\n",
    "        # Generate LDA model\n",
    "        print('--------- %s ---------'%str(i))\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word = dictionary, passes=60, alpha='auto',\n",
    "                            random_state=42)\n",
    "        shown_topics = ldamodel.show_topics(num_topics=i, \n",
    "                                             num_words=n_words,\n",
    "                                             formatted=False)\n",
    "        topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "        # Compute Coherence Score\n",
    "        coherence_ldamodel = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_ldamodel.get_coherence()\n",
    "        coherence[i] = coherence_lda\n",
    "        print('Coherence Score: ', coherence_lda)\n",
    "\n",
    "    return topics, coherence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create Tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Create PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # Save topics to CSV\n",
    "    data_path = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\\merged_all_data\\journal+doi+abstract+year+citation+fieldofstudy_dropnull.csv\"\n",
    "    out_path = r'C:\\Users\\hn0139\\Documents\\GitHub\\Misinformation\\Data analysis\\Content analysis\\topics.csv'\n",
    "    \n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f:    \n",
    "        data = pd.read_csv(f)\n",
    "\n",
    "    join_fn = lambda x : ' '.join([x.title, x.abstract])\n",
    "    join_text = data.apply(join_fn, axis=1)\n",
    "\n",
    "    clean_text = [preprocessing(tokenizer, text) for text in join_text]\n",
    "    \n",
    "    topics, coherences = get_lda_topic (clean_text, n_topics = [30], n_words = 10) # 25, 30, 35, 40\n",
    "    best_cor_score = max(list(coherences.values()))\n",
    "    best_cor_idx = list(coherences.values()).index(best_cor_score)\n",
    "    best_n_topics = {v:k for k, v in coherences.items()}[best_cor_score]\n",
    "\n",
    "    best_topics = pd.DataFrame(topics[best_n_topics])\n",
    "    print('best n_topic: {} topics. Cohenrence score: {} \\nTopics: \\n{}'.format(best_n_topics, coherences, best_topics))\n",
    "    with open (out_path, 'w',  newline=\"\", encoding='utf-8') as file:\n",
    "        best_topics.to_csv(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b141cd",
   "metadata": {},
   "source": [
    "# Topic evolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc1771ffc13a1574a693477487a0f073403269b3ea89b7de2bb07c89feb8bd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
