{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d648ad",
   "metadata": {},
   "source": [
    "# Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1673bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'authors', 'venue', 'year', 'citationCount',\n",
      "       'fieldsOfStudy', 'abstract', 'doi', 'query', 'database',\n",
      "       'publication_type'],\n",
      "      dtype='object')\n",
      "[('social media', 9.924303411959038e-07), ('fake news', 1.3047250484093075e-06), ('social media platforms', 2.5632053191541136e-06), ('fake news detection', 2.640716614146843e-06), ('online social networks', 3.395765128235662e-06), ('social media use', 4.5968016736445965e-06), ('social media users', 5.411398006995505e-06), ('social networks', 6.61669595737636e-06), ('social media news', 8.450279803841577e-06), ('social', 8.499241242615078e-06), ('information', 8.783742531554572e-06), ('social media information', 9.109590952449133e-06), ('online social media', 9.394470102162478e-06), ('using social media', 9.861601659799538e-06), ('news', 1.0067561475624971e-05), ('media', 1.251527732559419e-05), ('social media networks', 1.2851504092180756e-05), ('misinformation', 1.4061149016683143e-05), ('social media data', 1.4357904744197948e-05), ('social media misinformation', 1.4497503775720887e-05), ('social media rumor', 1.5035576505464512e-05), ('online fake news', 1.5447548406878292e-05), ('public health', 1.686749120318783e-05), ('social media content', 1.7492105918025996e-05), ('study', 1.8544175445586455e-05), ('used social media', 1.9485397226633535e-05), ('rumor spreading model', 1.9977059268223855e-05), ('social network analysis', 2.0858260990477103e-05), ('online social', 2.1789118445120297e-05), ('fake', 2.293005370776921e-05), ('health information', 2.3527313254290187e-05), ('detect fake news', 2.563220200529922e-05), ('news media', 2.6507024485196167e-05), ('social media posts', 2.689679174620373e-05), ('results', 2.9958481541827125e-05), ('health', 3.0394881722687434e-05), ('rumor propagation model', 3.1127848040802724e-05), ('spam', 3.2048922867267484e-05), ('model', 3.3100429988677664e-05), ('new', 3.318566560483787e-05), ('news detection', 3.358723551693743e-05), ('social networks social', 3.5021053730882586e-05), ('spam detection', 3.543759307978592e-05), ('news social media', 3.59136891663267e-05), ('fake news social', 3.65736632308584e-05), ('social media social', 3.7765470658262826e-05), ('public health information', 3.8780924001039674e-05), ('rumor', 3.94530751434657e-05), ('via social media', 3.9732779948503445e-05), ('online health information', 4.020433436238659e-05), ('health misinformation', 4.024029892208074e-05), ('social media using', 4.0542140156953655e-05), ('results show', 4.5147702391172216e-05), ('media social media', 4.551956969862823e-05), ('fake news online', 4.80590394880658e-05), ('public', 4.807196689869554e-05), ('media platforms', 4.890237103269411e-05), ('detecting fake news', 4.9519831101230186e-05), ('false information', 4.968718902340935e-05), ('online', 5.0604697688844115e-05), ('data', 5.114391503688747e-05), ('analysis', 5.190124189913893e-05), ('fake news spreading', 5.2147640774124185e-05), ('spreading fake news', 5.214764077412419e-05), ('using', 5.2664550246088076e-05), ('different social media', 5.348030270389168e-05), ('research', 5.409975631299479e-05), ('among social media', 5.451533330978138e-05), ('fake news research', 5.560004621429917e-05), ('rumors', 5.56219143601562e-05), ('fake news spread', 5.5789569459458474e-05), ('new media', 5.613252243923895e-05), ('news fake news', 5.796538622087688e-05), ('social media based', 5.8961400129044786e-05), ('misinformation effect', 5.941451303959957e-05), ('social media analysis', 5.984074131631809e-05), ('social media companies', 6.0436876695988885e-05), ('rumor detection model', 6.060097231495443e-05), ('social media sites', 6.090771071392683e-05), ('rumor detection', 6.120872443810658e-05), ('share fake news', 6.128004618272604e-05), ('vaccine', 6.132541008276274e-05), ('rumor spreading', 6.167735896457248e-05), ('online health misinformation', 6.233220262148384e-05), ('social network information', 6.283168434483707e-05), ('methods', 6.377717382035352e-05), ('social media may', 6.385030675712637e-05), ('fake health news', 6.771414934972221e-05), ('networks', 6.773762648975029e-05), ('online news media', 6.788701303972434e-05), ('news detection using', 6.845091152021114e-05), ('based', 7.040806517404229e-05), ('fake news related', 7.063536389391162e-05), ('network', 7.084802770611638e-05), ('fake news based', 7.145488237961703e-05), ('social media research', 7.34200794672192e-05), ('media use', 7.382957806130184e-05), ('networks social media', 7.389614853003935e-05), ('social networks using', 7.419516952311587e-05), ('social network spam', 7.459679325424827e-05)]\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import yake\n",
    "import nltk\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def yake_extractor(data_list, lemmatizer, stopwords, language = \"en\", max_ngram_size = 3, deduplication_thresold = 0.9, \n",
    "    deduplication_algo = 'seqm', windowSize = 1, numOfKeywords =100):\n",
    "    # preprocess data\n",
    "    lower_data = [data.lower() for data in data_list]\n",
    "    lem_data = [lemmatizer.lemmatize(data) for data in lower_data]\n",
    "    # text = []\n",
    "    # for data in lem_data:\n",
    "    #     text.append(' '.join([w for w in data.split() if w not in stopwords]))\n",
    "    text = ' '.join(lem_data)\n",
    "    # extract keywords\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size,  dedupLim=deduplication_thresold,\n",
    "                            dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None, stopwords=stopwords)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    return keywords\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stopwords = set(STOPWORDS)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    data_path = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\\merged_all_data\\journal+doi+abstract+year+citation+fieldofstudy_dropnull.csv\"\n",
    "    out_path = r'C:\\Users\\hn0139\\Documents\\GitHub\\Misinformation\\Data analysis\\Content analysis\\keyword.csv'\n",
    "    \n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f:    \n",
    "        data = pd.read_csv(f)\n",
    "    print(data.columns)\n",
    "\n",
    "    join_fn = lambda x : ' '.join([x.title, x.abstract])\n",
    "    join_text = data.apply(join_fn, axis=1)\n",
    "    keywords = yake_extractor(data_list = join_text.tolist(), lemmatizer = lemmatizer, stopwords = stopwords)\n",
    "    \n",
    "    # write output to file\n",
    "    f = open(out_path, 'w')\n",
    "    f.writelines('keyword,score\\n')\n",
    "    for (w, s) in keywords: \n",
    "        f = open(out_path, 'a+')\n",
    "        f.write('%s,%f\\n' %(w,s))   \n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22296d",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c39d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 20 ---------\n",
      "Coherence Score:  0.4645722276139618\n",
      "--------- 25 ---------\n",
      "Coherence Score:  0.470628677985547\n",
      "--------- 30 ---------\n",
      "Coherence Score:  0.4849451488002597\n",
      "--------- 35 ---------\n",
      "Coherence Score:  0.46380498878700577\n",
      "--------- 40 ---------\n",
      "Coherence Score:  0.47095674013627525\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict_values' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hn0139\\Documents\\GitHub\\Misinformation\\Data analysis\\Content analysis\\Content analysis.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m join_text \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mapply(join_fn, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m clean_text \u001b[39m=\u001b[39m [preprocessing(tokenizer, text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m join_text]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m topics \u001b[39m=\u001b[39m get_lda_topic (clean_text, n_topics \u001b[39m=\u001b[39;49m [\u001b[39m20\u001b[39;49m, \u001b[39m25\u001b[39;49m, \u001b[39m30\u001b[39;49m, \u001b[39m35\u001b[39;49m, \u001b[39m40\u001b[39;49m], n_words \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\hn0139\\Documents\\GitHub\\Misinformation\\Data analysis\\Content analysis\\Content analysis.ipynb Cell 4\u001b[0m in \u001b[0;36mget_lda_topic\u001b[1;34m(texts, n_topics, n_words)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCoherence Score: \u001b[39m\u001b[39m'\u001b[39m, coherence_lda)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m best_cor_score \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlist\u001b[39m(coherence\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m best_cor_idx \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(coherence\u001b[39m.\u001b[39;49mvalues()\u001b[39m.\u001b[39;49mindex(best_cor_score))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Get topics from results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# topics = []\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# for topic in ldamodel.print_topics(num_topics=n_topics, num_words=n_words):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m#     topics.append(topic)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# print(topics)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# return topics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hn0139/Documents/GitHub/Misinformation/Data%20analysis/Content%20analysis/Content%20analysis.ipynb#W6sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(coherence)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict_values' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# from stop_words import get_stop_words\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "'''# Create a list of tokenized and stemmed documents\n",
    "texts = []\n",
    "for i in abstract_set:\n",
    "    # Lowercase tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # Remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop + stop_plus]\n",
    "    # Stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens if len(i)>3]\n",
    "    texts.append(stemmed_tokens)'''\n",
    "\n",
    "def preprocessing (tokenizer, text):\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    \"\"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "     # Remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stopwords]\n",
    "    # Stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens if len(i)>3]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def get_lda_topic (texts, n_topics =[20, 25, 30, 35, 40], n_words = 12):\n",
    "    \"\"\"\n",
    "    texts: a list of token lists\n",
    "    \"\"\"\n",
    "    # Turn our tokenized documents into a id - term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # Convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Find the best n_topics\n",
    "    models = {}\n",
    "    topics = {} \n",
    "    coherence = {}\n",
    "    for i in n_topics:\n",
    "        # Generate LDA model\n",
    "        print('--------- %s ---------'%str(i))\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word = dictionary, passes=60, alpha='auto',\n",
    "                            random_state=42)\n",
    "        shown_topics = ldamodel.show_topics(num_topics=i, \n",
    "                                             num_words=n_words,\n",
    "                                             formatted=False)\n",
    "        topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "        # Compute Coherence Score\n",
    "        coherence_ldamodel = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_ldamodel.get_coherence()\n",
    "        coherence[i] = coherence_lda\n",
    "        print('Coherence Score: ', coherence_lda)\n",
    "    \n",
    "    best_cor_score = max(list(coherence.values()))\n",
    "    best_cor_idx = list(coherence.values()).index(best_cor_score)\n",
    "\n",
    "    # Get topics from results\n",
    "    # topics = []\n",
    "    # for topic in ldamodel.print_topics(num_topics=n_topics, num_words=n_words):\n",
    "    #     topics.append(topic)\n",
    "    # print(topics)\n",
    "    # return topics\n",
    "    print(coherence)\n",
    "    print(best_cor_idx, best_cor_score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create Tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Create PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # Save topics to CSV\n",
    "    data_path = r\"C:\\Users\\hn0139\\OneDrive - UNT System\\A_PhD_PATH\\PROJECTS\\Misinformation\\Misinformation_literature_review\\metadata\\merged_all_data\\journal+doi+abstract+year+citation+fieldofstudy_dropnull.csv\"\n",
    "    out_path = r'C:\\Users\\hn0139\\Documents\\GitHub\\Misinformation\\Data analysis\\Content analysis\\topics.csv'\n",
    "    \n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f:    \n",
    "        data = pd.read_csv(f)\n",
    "\n",
    "    join_fn = lambda x : ' '.join([x.title, x.abstract])\n",
    "    join_text = data.apply(join_fn, axis=1)\n",
    "\n",
    "    clean_text = [preprocessing(tokenizer, text) for text in join_text]\n",
    "    \n",
    "    topics = get_lda_topic (clean_text, n_topics = [20, 25, 30, 35, 40], n_words = 10)\n",
    "\n",
    "    # df_topics = pd.DataFrame(topics)\n",
    "    # print(df_topics)\n",
    "    # with open (out_path, 'w',  newline=\"\", encoding='utf-8') as file:\n",
    "    #     df_topics.to_csv(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b141cd",
   "metadata": {},
   "source": [
    "# Topic evolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc1771ffc13a1574a693477487a0f073403269b3ea89b7de2bb07c89feb8bd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
